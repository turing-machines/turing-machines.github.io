[
{
	"uri": "https://docs.turingpi.com/turing_pi/children/specs/",
	"title": "Specs",
	"tags": ["specs", "turing_pi"],
	"description": "",
	"content": "Turing Pi specifications\nScheme Overview    Name Value     Supported Compute Modules Raspberry Pi Compute Module 1, 3, 3+   CPU cores, Max. 28   RAM, Max. 7GB   Internal eMMC, Max. 224GB   Boot from network (netboot) Yes, tested with CM3, CM3+ and Hypriot OS v1.11.3   Boot from eMMC Yes   Boot from SD Yes   Compute Modules, Max. QTY 7x, DDR2 SO-DIMM 200 pin   Micro SD slots, QTY 7x, 1 per node   Ethernet Port 1x, 1Gbps   Int. Network Speed, Max. 1Gbps   Node Network Speed 100Mbps per node   Power 12-18 V, 3-5 A   Power consumption, Max. 40 W   Board form factor Mini ITX, 170 x 170 mm   Mini ITX Power socket 2x2 pin 12 V   Flash port Only for Master Node via micro USB   GPIO 40-pin, RPI compatible pinout 7x   HDMI 1x, Node #1 (Master Node)   Audio 3.5 mm 1x, Node #1 (Master Node)   USB 2.0 8x    Cluster Management Bus (CMB)    Name Value     CMB protocol I2C   CMB access From each node   Node Power Management Yes, via CMB for each node   User space EEPROM 128bytes   Int. CMB devices Ethernet Switch, I2C expander and RTCC   External I2C ports 1x, for additional devices like LCD displays or EEPROM    Block Scheme Reference Links  Cluster Management Bus Buy Turing Pi "
},
{
	"uri": "https://docs.turingpi.com/turing_pi/",
	"title": "Turing Pi Basics",
	"tags": [],
	"description": "This is Turing Pi Basics",
	"content": " Basics  Specs  Turing Pi specifications\n Cluster Management Bus  Turing Pi cluster management bus configuration, security and internal devices\n Assembly  This page contains tutorial to build a Turing Pi cluster\n "
},
{
	"uri": "https://docs.turingpi.com/docker_kubernetes/children/k8s_install/",
	"title": "Creating a Turing Pi Kubernetes Cluster",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "Building Kubernetes on top of Turing Pi brings another dimension to the edge computing and learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7.\nKey Aspects  Assemble a Turing Pi Cluster Deploy Kubernetes on Turing Pi  Prepare master and workers sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\ sudo apt-key add - \u0026amp;\u0026amp; echo \u0026quot;deb http://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; | \\ sudo tee /etc/apt/sources.list.d/kubernetes.list \u0026amp;\u0026amp; sudo apt-get update -q sudo apt-get install -qy kubelet kubectl kubeadm  To help during the initialization phase, get kubeadm to download the images onto docker\nkubeadm config images pull  Initialize the Kubernetes master node sudo kubeadm init --token-ttl=0 --pod-network-cidr=10.244.0.0/16 [init] using Kubernetes version: ... [preflight] running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.04.0-ce. Latest validated version: 18.06 [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [preflight] Activating the kubelet service [certificates] Generated ca certificate and key. [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [kubemaster-pi kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.2.1] [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [192.168.2.1 127.0.0.1 ::1] [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] valid certificates and keys now exist in \u0026quot;/etc/kubernetes/pki\u0026quot; [certificates] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/admin.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/controller-manager.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/scheduler.conf\u0026quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; [etcd] Wrote Static Pod manifest for a local etcd instance to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 88.010108 seconds [uploadconfig] storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.12\u0026quot; in namespace kube-system with the configuration for the kubeletsin the cluster [markmaster] Marking the node kubemaster-pi as master by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot; [markmaster] Marking the node kubemaster-pi as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;kubemaster-pi\u0026quot; asan annotation [bootstraptoken] using token: vej1mx.6qf2xljr39rr1i14 [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx  Initialize kubectl configuration As normal user (not root)\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes  Add worker nodes kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx  Setup Container Network Interface (CNI) At that point, nodes are not in ready state yet and CoreDNS in pending state\nkubectl get nodes kubectl get all -n kube-system  The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  The nodes should turn ready and CoreDNS should start\nkubectl get nodes kubectl get all -n kube-system  5 node cluster kubectl get nodes NAME STATUS ROLES AGE VERSION kube-node01 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kube-node02 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kube-node03 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kube-node04 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kubemaster-pi Ready master 23d v1.9.8  kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/etcd-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-controller-manager-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-dns-7b6ff86f69-l7lf6 3/3 Running 0 23d kube-system pod/kube-flannel-ds-8xbx4 1/1 Running 0 23d kube-system pod/kube-flannel-ds-9cz9f 1/1 Running 0 23d kube-system pod/kube-flannel-ds-rgpcq 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xnjtz 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xxdf6 1/1 Running 0 23d kube-system pod/kube-proxy-5m95q 1/1 Running 0 23d kube-system pod/kube-proxy-7sh7m 1/1 Running 0 23d kube-system pod/kube-proxy-f7t9r 1/1 Running 0 23d kube-system pod/kube-proxy-pkqvd 1/1 Running 0 23d kube-system pod/kube-proxy-shrdr 1/1 Running 0 23d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 0 23d kube-system pod/kubernetes-dashboard-7fcc5cb979-8vbmp 1/1 Running 0 23d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 23d kube-system service/kubernetes-dashboard NodePort 10.102.144.189 \u0026lt;none\u0026gt; 443:30383/TCP 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.extensions/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.extensions/kube-proxy 5 5 5 5 5 \u0026lt;none\u0026gt; 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.extensions/kube-dns 1 1 1 1 23d kube-system deployment.extensions/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.extensions/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.extensions/kubernetes-dashboard-7fcc5cb979 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 \u0026lt;none\u0026gt; 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/kube-dns 1 1 1 1 23d kube-system deployment.apps/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.apps/kubernetes-dashboard-7fcc5cb979 1 1 1 23d  Cleanup Teardown cluster\nsudo kubeadm reset sudo docker rm $(sudo docker ps -qa) sudo docker image rm $(sudo docker image list -qa) sudo apt-get purge kubeadm kubectl kubelet sudo apt-get autoremove sudo rm -rf ~/.kube  Reference Links  Kubeadm on Hypriot "
},
{
	"uri": "https://docs.turingpi.com/turing_pi/children/i2c_cluster_bus/",
	"title": "Cluster Management Bus",
	"tags": ["i2c", "cmb", "rtc", "turing_pi"],
	"description": "",
	"content": "Turing Pi cluster management bus configuration, security and internal devices\nConfigure I2C and devices Add dtoverlay to /boot/config.txt and save\ndtoverlay=i2c1-bcm2708,sda1_pin=44,scl1_pin=45,pin_func=6 dtoverlay=i2c-rtc,mcp7940x  Enable I2C interface in raspi-config -\u0026gt; Interfacing Options -\u0026gt; I2C and reboot\nCheck everything is fine with I2C and RTC\ndmesg | grep i2c [ 4.414436] i2c /dev entries driver # ok ls /dev/*i2c* /dev/i2c-1 # ok dmesg | grep rtc [ 6.489206] rtc-ds1307 1-006f: registered as rtc0 ls /dev/*rtc* /dev/rtc /dev/rtc0  I2C tools Install i2c-tools\nsudo apt-get install i2c-tools  Now check our I2C devices on the CMB\nsudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- 04 -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- 57 -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 6f 70: -- -- -- -- -- -- -- --  You can see three I2C devices Ethernet Switch (0x04h), I2C expander (0x57h) and RTCC (0x6Fh).\nThe i2cget command is used to read a byte from a specified register on the I2C device. The format for this command is as follows\ni2cget [-f] [-y] 1 \u0026lt;DEVICE_ADDRESS\u0026gt; \u0026lt;ADDRESS\u0026gt; [MODE]  [-f] [-y] Options:\n -f force access to the device even if the device is still busy. However, be careful. This can cause the i2cget command to return an invalid value. We recommend avoiding this option. -y disable interactive mode. Using this command will skip the prompt for confirmation from the i2cget command. By default, it will wait for the confirmation from the user before interacting with the I2C device. DEVICE_ADDRESS is an I2C bus address (e.g 0x04h) ADDRESS is the address on the slave from which to read data (eg. 0x00h)  The optional MODE can be one of the following:\n b – read/write a byte of data, this is the default if left blank w – read/write a word of data (two bytes) c – write byte/read byte transaction  Example of reading from register 0xF2h where factory default value is 0xFFh\nsudo i2cget -y 1 0x57 0xf2 0xff  Use the i2cset command to write data to an I2C device. The format of this command as follows\ni2cset [-m mask] [-f] [-y] 1 \u0026lt;DEVICE_ADDRESS\u0026gt; \u0026lt;ADDRESS\u0026gt; \u0026lt;VALUE\u0026gt; [MODE]  Options:\n -f, -y, DEVICE_ADDRESS, ADDRESS as for i2cget -m mask the bit mask parameter, if specified, describes which bits of value will be actually written to data-address. Bits set to 1 in the mask are taken from VALUE, while bits set to 0 will be read from ADDRESS and thus preserved by the operation  Example of writing value 0x77h to the first byte of user space SRAM (starts from 0x00h) and then reading\nsudo i2cset -y 1 0x57 0x00 0x77 sudo i2cget -y 1 0x57 0x00 0x77  Security As a security precaution, system devices are not exposed by default inside Docker containers. You can expose specific devices to your container using the --device option to docker run, as in\ndocker run --device /dev/i2c-1 app-image  or remove all restrictions with the --privileged flag\ndocker run --privileged app-image  User space EEPROM I2C expander and RTCC each have 64 bytes of general purpose user memory, so combined you have 128 bytes of EEPROM. On device 0x57h user space address from 0x00h to 0x3Fh and on 0x6Fh from 0x20h to 0x5Fh.\nEthernet Switch The onboard RTL8370 integrate all the functions of a high-speed switch system; including SRAM for packet buffering, non-blocking switch fabric, and internal register management into a single CMOS device.\nDevice Address: 0x04h\n    Register Register Description Default Value     0x00h Control Register 0x1140h   0x01h Status Register 0x7949h   0x02h PHY Identifier 1 0x001Ch   0x03h PHY Identifier 2 0xC980h   0x04h Auto-Negotiation Advertisement Register 0x0DE1h   0x05h Auto-Negotiation Link Partner Ability Register 0x0000h   0x06h Auto-Negotiation Expansion Register 0x0004h   0x07h Auto-Negotiation Page Transmit Register 0x2001h   0x08h Auto-Negotiation Link Partner Next Page Register 0x0000h   0x09h 1000Base-T Control Register 0x0E00h   0x0Ah 1000Base-T Status Register 0x0000h    Network Speed # 0x2040: Bitmask # 0x2000: 1000Mbps # 0x0040: 100Mbps # 0x0000: 10Mbps # Set network speed to 1000Mbps sudo i2cset -m 0x2040 -y 0x04 0x00 0x2000 w  Power Up/Down # 0x0800: Bitmask # 0x0800: Power down # 0x0000: Normal operation # Power down sudo i2cset -m 0x0800 -y 0x04 0x00 0x0800 w # Power up sudo i2cset -m 0x0800 -y 0x04 0x00 0x0000 w  I2C expander I2C expander offers users a digitally programmable alternative to hardware jumpers and mechanical switches that are being used to control power on compute nodes.\nDevice Address: 0x57h\n    Register Register Description Default Value     0x00h to 3Fh 64 bytes of general-purpose user EEPROM 0x00h   0xF2h Control Register 0xFFh   0xF8h Status Register \u0026ndash;   0xFAh to 0xFFh 6 bytes of general-purpose user SRAM \u0026ndash;    Power Management On/off compute module through register 0xF2h\n# Bits : 76543210 # Slots : 5674321x # Power off worker nodes 2,3,4,5,6,7 sudo i2cset -m 0xfc -y 1 0x57 0xf2 0x00 # Power on worker nodes sudo i2cset -m 0xfc -y 1 0x57 0xf2 0xff  Read on/off status\nsudo i2cget -y 1 0x57 0xf2 0xff  Compute Module status Read compute module status through register 0xF8h\n 1 = Installed in slot AND switched on 0 = Not installed in slot OR switched off  # Bits : 76543210 # Slots : 5674321x sudo i2cget -y 1 0x57 0xf8 0x0e # register 0xf2 = 0xff, all slots are power up but installed only three  Real-Time Clock/Calendar Real-Time Clock/Calendar (RTCC) tracks time using internal counters for hours, minutes, seconds, days, months, years, and day of week. Alarms can be configured on all counters up to and including months.\nSRAM and timekeeping circuitry are powered from the back-up supply when main power is lost, allowing the device to maintain accurate time and the SRAM contents. The times when the device switches over to the back-up supply and when primary power returns are both logged by the power-fail time-stamp.\nDevice Address: 0x6Fh\n    Register Register Description Default Value     0x00h to 0x06h Time \u0026amp; Date \u0026ndash;   0x07h to 0x09h Configuration and Trimming \u0026ndash;   0x0Ah to 0x10h Alarm 0 \u0026ndash;   0x11h to 0x17h Alarm 1 \u0026ndash;   0x18h to 0x1Fh Power-Fail/Power-Up Time-Stamps \u0026ndash;   0x20h to 0x5Fh 64 bytes of general-purpose user SRAM \u0026ndash;    Time \u0026amp; Date Show time from hardware RTC\nsudo hwclock --show 2019-10-26 20:21:03.005326+01:00  External I2C port Pinouts\n| 1 | 2 | 3 | 4 | | GND | VCC | SCL | SDA |  Reference Links  I2C tools I2C expander datasheet RTC datasheet "
},
{
	"uri": "https://docs.turingpi.com/maintenance/children/helm-install/",
	"title": "Deploy Helm and Tiller on Turing PI cluster",
	"tags": ["kubernetes", "helm"],
	"description": "",
	"content": "Use Helm \u0026amp; Tiller on the Turing PI Cluster.\nInstall Helm wget https://get.helm.sh/helm-v2.14.3-linux-arm.tar.gz tar xvzf helm-v2.14.3-linux-arm.tar.gz sudo mv linux-arm/helm /usr/local/bin/helm rm -rf linux-arm  Create rbac-config.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system  kubectl apply -f rbac-config.yaml  Init Helm\nhelm init --tiller-image=jessestuart/tiller --service-account tiller --override spec.selector.matchLabels.'name'='tiller',spec.selector.matchLabels.'app'='helm' --output yaml | sed 's@apiVersion: extensions/v1beta1@apiVersion: apps/v1@' | kubectl apply -f -  Reference Links  Usage of remote API "
},
{
	"uri": "https://docs.turingpi.com/docker_kubernetes/children/2018-07-08-a/",
	"title": "Add Persistency Volume to Turing Pi",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes\nProcedures The procedure assumes that you have 32G SD Card that you can partition into one 16G disk and seven 7G disks.\nPartition the SD cards Use the Rescue dongle to create the partition. The ideal solution would be to update the cloud-init of the HypriotOS to do that automatically.\nMount the disks Mounting the partitions, involves creating new directories on each node and adding \u0026ldquo;.mount\u0026rdquo; files under systemd This kind of done automatically under by the mount-disks playbook\ncd $HOME/mgt/ ansible picluster -i inventory/ -m shell -a \u0026quot;df -kh\u0026quot; ansible-playbook -i inventory/ playbooks/mount_disks.yaml ansible picluster -i inventory/ -m shell -a \u0026quot;df -kh\u0026quot;  Create the Persistency Volumes in Kubernetes The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment\nBecause we did not convert the go code to create the volumes automatically from the /mnt/disks/\u0026hellip;, we have to kind of create the PV by hand.\ncd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd $HOME/kube-deployment/local-storage kubectl apply -f local-storageclass.yaml kubectl apply -f admin_account.yaml kubectl apply -f kubemaster-pi-volumes.yaml kubectl apply -f kube-node01-volumes.yaml kubectl apply -f kube-node02-volumes.yaml kubectl apply -f kube-node03-volumes.yaml kubectl apply -f kube-node04-volumes.yaml  Results    Reference Links  TBD "
},
{
	"uri": "https://docs.turingpi.com/docker_kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "This is Kubernetes tutorial page",
	"content": " Docker and Kubernetes $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6955765f44-slbhm 1/1 Running 19 8h kube-system coredns-6955765f44-zv8f8 1/1 Running 19 8h kube-system etcd-master 1/1 Running 38 8h kube-system kube-apiserver-master 1/1 Running 28 8h kube-system kube-controller-manager-master 1/1 Running 29 8h kube-system kube-flannel-ds-arm-68ts8 1/1 Running 7 7h27m kube-system kube-flannel-ds-arm-dflh6 1/1 Running 14 7h48m kube-system kube-flannel-ds-arm-hjh67 1/1 Running 25 7h59m kube-system kube-flannel-ds-arm-q6d7q 1/1 Running 16 7h32m kube-system kube-flannel-ds-arm-qp4fq 1/1 Running 0 7h40m kube-system kube-proxy-62lm8 1/1 Running 12 7h32m kube-system kube-proxy-9fdnh 1/1 Running 0 7h40m kube-system kube-proxy-hxm2m 1/1 Running 6 7h27m kube-system kube-proxy-vb49r 1/1 Running 20 8h kube-system kube-proxy-zpbmc 1/1 Running 12 7h48m kube-system kube-scheduler-master 1/1 Running 30 8h   Creating a Turing Pi Kubernetes Cluster  Building Kubernetes on top of Turing Pi brings another dimension to the edge computing and learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7.\n Add Persistency Volume to Turing Pi  In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes\n Deploy Flannel in Turing Pi  In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0).\n Add RPI Compute Module node to Turing Pi in 10 min  During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work.\n "
},
{
	"uri": "https://docs.turingpi.com/",
	"title": "Turing Machines",
	"tags": [],
	"description": "This is Turing Machines welcome page",
	"content": "Turing Machines Personal, Mobile Edge Computers and Lab Turing Pi is personal and mobile edge cloud. The key concept is to leverage the 7 network interfaces available on each Raspberry Pi Compute Module.\nThe Raspberry Pi Compute Modules are interconnected together using the onboard 1 Gbps switch. The Turing top/master node is acting as a NAT/Router for the rest of the compute modules. This main advantage that when you move your cluster from home to work, all the IPs of the cluster stay identical.\nNext step is to install the OS. Immediatly you will realize that it is unpracticle to plug each node one after the other to an HDMI screen, USB mouse and keyboard in order to perform the first initalization. Some of the OS such Hypriot OS have the great idea to preconfigure the node with SSH enabled, docker. This makes every plug and play: Flush your SD card, connect the PI to your home router and voila you just have to SSH.\nThen you will have to learn how to create a NAT and DHCP server on your master node.\nThe bigger your cluster the quicker you will realize that some of the operations are repetitiv, hence the need for automation. The easiet one to learn is ansible. Put if you install everything on the OS directly, then start the issue of maintaining the OS. Because Hypriot OS comes by default with docker, which brings us back to lesson 1: Have everything has a container.\nThe defacto tools to manage a multi node docker cluster is currently kubernetes. By using kubeadm the installation is quite simple as long as you pay attention that the images pulled by kubernetes are actually the one for PI and not the one for your usual cloud.\nFor people with Kubernetes experience (for instance on GCP), this is where the real lessons start: - A lot of the software is not available by default on PI. How do you recompile calico, tiller\u0026hellip;for the PI. - It is much easier to install components using the Kubernetes HELM but a lot of the helm charts are pulling the AMD64 version of the ARM image. - Finally, how do you create your own helm chart repository using github so that you can run a clean helm add repo and helm install commands.\nAnd mainly, you will started to appreciate go and the new Java 9 modules to actually create true microservices Because both language have the ability to create standalone executables, which in turn allow to create containers starting from \u0026ldquo;scratch\u0026rdquo;. Compare it with a python container and you will understand very quickly why it is beneficial. The PI only has 1G of RAM\u0026hellip;which helps to understand the interest of having slim true microservices.\n Turing Pi Basics  This is Turing Pi Basics\n Kubernetes  This is Kubernetes tutorial page\n Helm \u0026amp; Maintenance  This is turing_pi/maintenance tutorial page\n FAQ  1. What model Raspberry Pi is in this? Supported models with eMMC and without Raspberry Pi Compute Module 1 Raspberry Pi Compute Module 3 Raspberry Pi Compute Module 3+ 2. Будет ли поддержка Raspberry Pi 4? Как только поступят в продажу Raspberry Pi 4 Compute Modules в короткие сроки будет версия платы с их поддержкой 3. How to the compute modules communicate with each other? We use onboard 1 Gbps Ethernet switch within the nodes.\n Specs  Turing Pi specifications\n Creating a Turing Pi Kubernetes Cluster  Building Kubernetes on top of Turing Pi brings another dimension to the edge computing and learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7.\n Cluster Management Bus  Turing Pi cluster management bus configuration, security and internal devices\n Deploy Helm and Tiller on Turing PI cluster  Use Helm \u0026amp; Tiller on the Turing PI Cluster.\n Add Persistency Volume to Turing Pi  In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes\n Deploy Flannel in Turing Pi  In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0).\n Upgrade Kubernetes cluster  Master node upgrade using kubeadm # apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-mark hold kubeadm kubeadm was already not hold. Hit:2 http://raspbian.raspberrypi.org/raspbian stretch InRelease Hit:3 https://download.docker.com/linux/raspbian stretch InRelease Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Hit:5 http://archive.raspberrypi.org/debian stretch InRelease Hit:4 https://packagecloud.io/Hypriot/rpi/debian stretch InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n Assembly  This page contains tutorial to build a Turing Pi cluster\n Add RPI Compute Module node to Turing Pi in 10 min  During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work.\n "
},
{
	"uri": "https://docs.turingpi.com/docker_kubernetes/children/2018-07-14-a/",
	"title": "Deploy Flannel in Turing Pi",
	"tags": ["kubernetes", "rpi", "cni"],
	"description": "",
	"content": "In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0).\nKey Aspects  Flannel seems to deploy ok. Looks like in trouble when multiple interfaces available Calico in not compiled by default for Rapsberry PI  Flannel Setup through kubectl The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment\ncd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd kube-deployment/flannel/ kubectl apply -f flannel.yaml ### Flannel Issue 1: flannel.1. Link has incompatible address on master-pi, both the WLAN and LAN interfaces were activated. After unplugging the CAT5, behavior was similar. Moreover this had some impact on the kube-apiserver (see the number of restarts). ```bash $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m 1/1 Running 0 4h kube-system pod/coredns-78fcdf6894-cw5p8 1/1 Running 15 10d kube-system pod/coredns-78fcdf6894-czjcj 1/1 Running 15 10d kube-system pod/etcd-master-pi 1/1 Running 11 10d kube-system pod/kube-apiserver-master-pi 1/1 Running 599 10d kube-system pod/kube-controller-manager-master-pi 1/1 Running 38 10d kube-system pod/kube-flannel-ds-bhllh 1/1 Running 13 9d kube-system pod/kube-flannel-ds-q7cp2 0/1 CrashLoopBackOff 401 9d kube-system pod/kube-flannel-ds-wqxsz 1/1 Running 16 9d kube-system pod/kube-proxy-4chwh 1/1 Running 9 9d kube-system pod/kube-proxy-6r5mn 1/1 Running 5 9d kube-system pod/kube-proxy-vvj6j 1/1 Running 11 10d kube-system pod/kube-scheduler-master-pi 1/1 Running 13 10d kube-system pod/kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 20 7d kube-system pod/tiller-deploy-b59fcc885-66l7s 1/1 Running 0 6h  $ kubectl logs pod/kube-flannel-ds-q7cp2 -n kube-system I0716 00:42:46.596796 1 main.go:474] Determining IP address of default interface I0716 00:42:46.598043 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:42:46.598138 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:42:46.775936 1 kube.go:283] Starting kube subnet manager I0716 00:42:46.775907 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:42:47.776280 1 kube.go:137] Node controller sync successful I0716 00:42:47.776400 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:42:47.776431 1 main.go:237] Installing signal handlers I0716 00:42:47.776697 1 main.go:352] Found network config - Backend type: vxlan I0716 00:42:47.776900 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0716 00:42:47.778884 1 main.go:279] Error registering network: failed to configure interface flannel.1: link has incompatible addresses. Remove additional addresses and try again.... I0716 00:42:47.778991 1 main.go:332] Stopping shutdownHandler...  Deleting the pod, did not help. After recreation same issue reappeared.\n$ kubectl delete pod/kube-flannel-ds-q7cp2 -n kube-system  $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 0/1 Error 1 17s  Deleting the interface flannel.1 interface actually worked:\n$ sudo ip link delete flannel.1  $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 1/1 Running 5 3m  $ kubectl logs pod/kube-flannel-ds-z7w4f -n kube-system I0716 00:52:14.555290 1 main.go:474] Determining IP address of default interface I0716 00:52:14.564490 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:52:14.564578 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:52:14.802491 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:52:14.802544 1 kube.go:283] Starting kube subnet manager I0716 00:52:15.803114 1 kube.go:137] Node controller sync successful I0716 00:52:15.803308 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:52:15.803909 1 main.go:237] Installing signal handlers I0716 00:52:15.804662 1 main.go:352] Found network config - Backend type: vxlan I0716 00:52:15.804985 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0716 00:52:15.875242 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0716 00:52:15.875367 1 main.go:303] Running backend. I0716 00:52:15.875489 1 main.go:321] Waiting for all goroutines to exit I0716 00:52:15.875559 1 vxlan_network.go:56] watching for new subnet leases  Flannel issue 2: Multiple interfaces Some of the PI have two interfaces running: wlan0 and eth0. The internal cluster network is using eth0. We need to force Flannel to use it.\nSetup through kubectl Realize I was using v0.9.1 instead of v0.10.0. Let\u0026rsquo;s update the file\n$ mkdir -p $HOME/kube-deployments/flannel $ cd $HOME/kube-deployments/flannel $ curl -sSL https://rawgit.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml | sed \u0026quot;s/amd64/arm/g\u0026quot; \u0026gt; flannel.yaml  Let\u0026rsquo;s add \u0026ndash;iface=eth0 to the flanneld to in the flannel.yaml\n Let\u0026rsquo;s update flannel from 0.9.1 to 0.10.0 at the same time we specify which interface to use.\n$ kubectl apply -f flannel.yaml clusterrole.rbac.authorization.k8s.io/flannel configured clusterrolebinding.rbac.authorization.k8s.io/flannel configured serviceaccount/flannel unchanged configmap/kube-flannel-cfg configured daemonset.extensions/kube-flannel-ds configured  It seems it solved the flannel issue. The bug in kube 1.11.0 still there (restart of kube-apiserver) Will update to 1.11.1 when it is published\n$ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-78fcdf6894-bn6wl 1/1 Running 0 6d kube-system pod/coredns-78fcdf6894-k52xb 1/1 Running 0 6d kube-system pod/etcd-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-controller-manager-kubemaster-pi 0/1 CrashLoopBackOff 1740 6d kube-system pod/kube-flannel-ds-62fz9 1/1 Running 984 6d kube-system pod/kube-flannel-ds-gwzdt 1/1 Running 0 6d kube-system pod/kube-flannel-ds-h7ln5 1/1 Running 0 6d kube-system pod/kube-flannel-ds-qs9lf 1/1 Running 0 6d kube-system pod/kube-flannel-ds-vwsjk 1/1 Running 0 6d kube-system pod/kube-proxy-45z5s 1/1 Running 0 6d kube-system pod/kube-proxy-4trsd 1/1 Running 0 6d kube-system pod/kube-proxy-ksj7c 1/1 Running 4 6d kube-system pod/kube-proxy-t7gmc 1/1 Running 0 6d kube-system pod/kube-proxy-tfmqb 1/1 Running 0 6d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 4 6d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6d kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 6d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2 2 2 2 6d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-78fcdf6894 2 2 2 6d  $ kubectl logs pod/kube-flannel-ds-62fz9 -n kube-system I0714 14:34:26.719081 1 main.go:474] Determining IP address of default interface I0714 14:34:26.728184 1 main.go:487] Using interface with name wlan0 and address 192.168.1.94 I0714 14:34:26.728273 1 main.go:504] Defaulting external address to interface address (192.168.1.94) I0714 14:34:26.942686 1 kube.go:283] Starting kube subnet manager I0714 14:34:26.943203 1 kube.go:130] Waiting 10m0s for node controller to sync I0714 14:34:27.943672 1 kube.go:137] Node controller sync successful I0714 14:34:27.943771 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - kubemaster-pi I0714 14:34:27.943819 1 main.go:237] Installing signal handlers I0714 14:34:27.944064 1 main.go:352] Found network config - Backend type: vxlan I0714 14:34:27.944222 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0714 14:34:28.004675 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0714 14:34:28.004748 1 main.go:303] Running backend. I0714 14:34:28.004880 1 main.go:321] Waiting for all goroutines to exit I0714 14:34:28.004931 1 vxlan_network.go:56] watching for new subnet leases I0714 14:34:28.049933 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.050202 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.053918 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.054003 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.057332 1 iptables.go:136] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.061665 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.066452 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.069910 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.075067 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE I0714 14:34:28.078310 1 iptables.go:124] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.082389 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.098375 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.111379 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.122424 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE  Calico Compile Calico for Raspberry PI  WIP  Deploy on Raspberry PI  WIP  Results  WIP  Reference Links  Flannel Issue Flannel Issue2 Flannel Issue3 "
},
{
	"uri": "https://docs.turingpi.com/maintenance/",
	"title": "Helm &amp; Maintenance",
	"tags": [],
	"description": "This is turing_pi/maintenance tutorial page",
	"content": "HELM and Maintenance Those tutorials describe how to use Helm and associated tools to perform rolling upgrades of the applications and of Kubernetes\n Deploy Helm and Tiller on Turing PI cluster  Use Helm \u0026amp; Tiller on the Turing PI Cluster.\n Upgrade Kubernetes cluster  Master node upgrade using kubeadm # apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-mark hold kubeadm kubeadm was already not hold. Hit:2 http://raspbian.raspberrypi.org/raspbian stretch InRelease Hit:3 https://download.docker.com/linux/raspbian stretch InRelease Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Hit:5 http://archive.raspberrypi.org/debian stretch InRelease Hit:4 https://packagecloud.io/Hypriot/rpi/debian stretch InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n "
},
{
	"uri": "https://docs.turingpi.com/maintenance/children/k8s-upgrade/",
	"title": "Upgrade Kubernetes cluster",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "Master node upgrade using kubeadm # apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-mark hold kubeadm kubeadm was already not hold. Hit:2 http://raspbian.raspberrypi.org/raspbian stretch InRelease Hit:3 https://download.docker.com/linux/raspbian stretch InRelease Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Hit:5 http://archive.raspberrypi.org/debian stretch InRelease Hit:4 https://packagecloud.io/Hypriot/rpi/debian stretch InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 38 not upgraded. Need to get 8,095 kB of archives. After this operation, 3,100 kB disk space will be freed. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubeadm armhf 1.12.0-00 [8,095 kB] Fetched 8,095 kB in 4s (1,962 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubeadm_1.12.0-00_armhf.deb ... Unpacking kubeadm (1.12.0-00) over (1.11.1-00) ... Setting up kubeadm (1.12.0-00) ... kubeadm set on hold.  sudo kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/versions] Latest stable version: v1.12.0 [upgrade/versions] Latest version in the v1.11 series: v1.11.3 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.11.3 Upgrade to the latest version in the v1.11 series: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.11.3 Controller Manager v1.11.0 v1.11.3 Scheduler v1.11.0 v1.11.3 Kube Proxy v1.11.0 v1.11.3 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.18 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.11.3 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.12.0 Upgrade to the latest stable version: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.12.0 Controller Manager v1.11.0 v1.12.0 Scheduler v1.11.0 v1.12.0 Kube Proxy v1.11.0 v1.12.0 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.24 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.12.0 _____________________________________________________________________  $ sudo kubeadm upgrade apply v1.12.0 [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file. [upgrade/version] You have chosen to change the cluster version to \u0026quot;v1.12.0\u0026quot; [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component kube-scheduler. [upgrade/prepull] Prepulling image for component etcd. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026quot;v1.12.0\u0026quot;... Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f [etcd] Wrote Static Pod manifest for a local etcd instance to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/etcd.yaml\u0026quot; [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/etcd.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 77c6076a4d6ee044b744b041125cf918 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \u0026quot;etcd\u0026quot; upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [util/etcd] Waiting 0s for initial delay [util/etcd] Attempting to see if all cluster endpoints are available 1/10 [upgrade/staticpods] Writing new Static Pod manifests to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315\u0026quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-apiserver.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-controller-manager.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-scheduler.yaml\u0026quot; [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-apiserver.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-apiserver-master-pi hash: a92106d6db4c8b5835a47f5f56c33fdb [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026quot;kube-apiserver\u0026quot; upgraded successfully! [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-controller-manager.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-controller-manager-master-pi hash: 980b4156606df8caafd0ad8abacc1485 [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026quot;kube-controller-manager\u0026quot; upgraded successfully! [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-scheduler.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: kube-scheduler-master-pi hash: 1b5ec5be325bf29f60be62789416a99e [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026quot;kube-scheduler\u0026quot; upgraded successfully! [uploadconfig] storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.12\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster [kubelet] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.12\u0026quot; ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [patchnode] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;master-pi\u0026quot; as an annotation [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026quot;v1.12.0\u0026quot;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.  Kubernetes servers are running v1.12\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;11\u0026quot;, GitVersion:\u0026quot;v1.11.1\u0026quot;, GitCommit:\u0026quot;b1b29978270dc22fecc592ac55d903350454310a\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-07-17T18:53:20Z\u0026quot;, GoVersion:\u0026quot;go1.10.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T16:55:41Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;}  my kubelets are still running kubernetes v1.11.1\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 86d v1.11.1 master-pi Ready master 86d v1.11.1 nas-pi Ready \u0026lt;none\u0026gt; 85d v1.11.1  Upgrade kubectl Install newer version of kubectl using apt-get\n$ sudo apt-get install kubectl Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubectl 1 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. Need to get 8,639 kB of archives. After this operation, 1,783 kB of additional disk space will be used. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubectl armhf 1.12.0-00 [8,639 kB] Fetched 8,639 kB in 6s (1,270 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubectl_1.12.0-00_armhf.deb ... Unpacking kubectl (1.12.0-00) over (1.11.1-00) ... Setting up kubectl (1.12.0-00) ...  Check that kubectl is now 1.12\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T17:05:32Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T16:55:41Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;}  Upgrade kubelet on master node $ sudo apt-get upgrade -y kubelet Reading package lists... Done Building dependency tree Reading state information... Done Calculating upgrade... Done The following packages will be upgraded: ...  $ sudo systemctl restart kubelet  $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 86d v1.11.1 master-pi Ready master 86d v1.12.0 nas-pi Ready \u0026lt;none\u0026gt; 85d v1.11.1  Looks that as useual something is wrong with flannel CNI\n$ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-6gvf6 1/1 Running 0 60d kube-system pod/coredns-576cbf47c7-nzhfj 1/1 Running 0 37m kube-system pod/coredns-576cbf47c7-wkmhr 1/1 Running 0 37m kube-system pod/etcd-master-pi 1/1 Running 0 3m43s kube-system pod/kube-apiserver-master-pi 1/1 Running 2 3m43s kube-system pod/kube-controller-manager-master-pi 1/1 Running 1 3m43s kube-system pod/kube-flannel-ds-4495g 1/1 Running 4 75d kube-system pod/kube-flannel-ds-52ssk 0/1 Error 10 75d kube-system pod/kube-flannel-ds-gj65n 1/1 Running 4 75d Investigation shows: ```bash $ kubectl logs pod/kube-flannel-ds-52ssk -n kube-system I0930 01:34:13.768925 1 main.go:475] Determining IP address of default interface I0930 01:34:13.778622 1 main.go:488] Using interface with name wlan0 and address 192.168.1.95 I0930 01:34:13.778716 1 main.go:505] Defaulting external address to interface address (192.168.1.95) I0930 01:34:14.168318 1 kube.go:131] Waiting 10m0s for node controller to sync I0930 01:34:14.168890 1 kube.go:294] Starting kube subnet manager I0930 01:34:15.169929 1 kube.go:138] Node controller sync successful I0930 01:34:15.170035 1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - master-pi I0930 01:34:15.170064 1 main.go:238] Installing signal handlers I0930 01:34:15.170415 1 main.go:353] Found network config - Backend type: vxlan I0930 01:34:15.170797 1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0930 01:34:15.256843 1 main.go:280] Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. \u0026amp;netlink.Vxlan{LinkAttrs:netlink.LinkAttrs{Index:5, MTU:1450, TxQLen:0, Name:\u0026quot;flannel.1\u0026quot;, HardwareAddr:net.HardwareAddr{0x26, 0xd5, 0xd0, 0xdd, 0x56, 0x1a}, ...  Let\u0026rsquo;s apply usual receipe ``bash sudo ip link delete flannel.1\n Brute force fix seems to have done the fix....This is lucky we don't have production traffic on that node. ```bash $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 51m coredns-576cbf47c7-wkmhr 1/1 Running 0 51m etcd-master-pi 1/1 Running 0 17m kube-apiserver-master-pi 1/1 Running 2 17m kube-controller-manager-master-pi 1/1 Running 1 17m kube-flannel-ds-4495g 1/1 Running 4 75d kube-flannel-ds-52ssk 1/1 Running 13 75d kube-flannel-ds-gj65n 1/1 Running 4 75d kube-proxy-c2264 1/1 Running 0 51m kube-proxy-snjsg 1/1 Running 0 49m kube-proxy-zgqjb 1/1 Running 1 50m kube-scheduler-master-pi 1/1 Running 1 17m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d  Update first slave node sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet  Same flannel issue\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 87m coredns-576cbf47c7-wkmhr 1/1 Running 1 87m etcd-master-pi 1/1 Running 0 53m kube-apiserver-master-pi 1/1 Running 2 53m kube-controller-manager-master-pi 1/1 Running 1 53m kube-flannel-ds-4495g 0/1 CrashLoopBackOff 10 75d ...  same hack to fix the issue\nsudo ip link delete flannel.1  same hack seems to be effective\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 111m coredns-576cbf47c7-wkmhr 1/1 Running 1 111m etcd-master-pi 1/1 Running 0 77m kube-apiserver-master-pi 1/1 Running 2 77m kube-controller-manager-master-pi 1/1 Running 1 77m kube-flannel-ds-4495g 1/1 Running 11 75d ...  Other slave node sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet sudo ip link delete flannel.1  Check consistency of the cluster $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 1 143m coredns-576cbf47c7-wkmhr 1/1 Running 1 143m etcd-master-pi 1/1 Running 0 109m kube-apiserver-master-pi 1/1 Running 2 109m kube-controller-manager-master-pi 1/1 Running 1 109m kube-flannel-ds-4495g 1/1 Running 11 76d kube-flannel-ds-52ssk 1/1 Running 13 76d kube-flannel-ds-gj65n 1/1 Running 13 76d kube-proxy-c2264 1/1 Running 1 143m kube-proxy-snjsg 1/1 Running 1 141m kube-proxy-zgqjb 1/1 Running 1 142m kube-scheduler-master-pi 1/1 Running 1 109m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d  $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 86d v1.12.0 master-pi Ready master 86d v1.12.0 nas-pi Ready \u0026lt;none\u0026gt; 86d v1.12.0  Conclusion  At a glance, the cluster seems to be healthy I still need to find sometool like sonobuoy to validate that the cluster is healthy  Reference Links  Official Upgrade documentation "
},
{
	"uri": "https://docs.turingpi.com/turing_pi/children/cluster_assembly/",
	"title": "Assembly",
	"tags": ["turing_pi", "rpi"],
	"description": "",
	"content": "This page contains tutorial to build a Turing Pi cluster\nRaspberry Pi Compute Module For cluster assembly you need to buy RPI compute modules at least tree for 3 node cluster\nAssembled 3 nodes Turing Pi 3 Nodes Cluster Hardware Assembly    Component URL Example Quantity Cost Estimate     Turing Pi clusterboard Clusterboard 1 $128   RPI Compute Module 3+ Compute Module 3 $105   Total   $233    5 Nodes Cluster Hardware Assembly    Component URL Example Quantity Cost Estimate     Turing Pi clusterboard Clusterboard 1 $128   RPI Compute Module 3+ Compute Module 5 $175   Total   $303    Reference Links  Video1 "
},
{
	"uri": "https://docs.turingpi.com/docker_kubernetes/children/2018-06-19-a/",
	"title": "Add RPI Compute Module node to Turing Pi in 10 min",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work.\nBase OS Flash HypriotOS to SD and reboot Pi.\nOS Flash the SD Card with HypriotOS Connect Pi through to the cluster switch  Freeze the new node IP Access the Master PI, run arp -a and find the new IP. Freeze the new IP in the dhcpcd.conf\nssh \u0026lt;masterip\u0026gt; arp -a vi /etc/dhcp/dhcpd.conf  Ssh to the new node Access the node from the Master PI:\nssh 192.168.2.xxx docker ps  Basic hostname Freeze your configuration\nsudo apt-get remove --purge cloud-init sudo apt-get autoremove  Set the host name\nsudo vi /etc/hosts sudo hostnamectl set-hostname \u0026lt;xxx\u0026gt;  Install kubeadm On Master PI Firt access the kubemaster node and regenerate a token (if you did not use ttl=0 when using kubeadm init on the master):\nkubeadm token create  On New Slave PI sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \u0026quot;deb http://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; \u0026gt; /etc/apt/sources.list.d/kubernetes.list apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm kubectl kubelet  To help during the initialization phase, get kubeadm to download the images onto docker\nkubeadm config images pull  Get the node to join the cluster\nkubeadm join 192.168.2.1:6443 --token yyyyyy.xxxx --discovery-token-ca-cert-hash sha256:zzzz  Conclusion  Will have to come back later and use cloud-init, create a clean \u0026amp; small SD image for Win32DiskImage Will have to create more advanced partition on the SD card. "
},
{
	"uri": "https://docs.turingpi.com/faq/",
	"title": "FAQ",
	"tags": ["turing_pi", "faq"],
	"description": "",
	"content": " 1. What model Raspberry Pi is in this? Supported models with eMMC and without\n Raspberry Pi Compute Module 1 Raspberry Pi Compute Module 3 Raspberry Pi Compute Module 3+  2. Будет ли поддержка Raspberry Pi 4? Как только поступят в продажу Raspberry Pi 4 Compute Modules в короткие сроки будет версия платы с их поддержкой\n3. How to the compute modules communicate with each other? We use onboard 1 Gbps Ethernet switch within the nodes. Also, each node can exchange some tech info thought the I2C bus with each other, including Real-Time Clock (RTC)\n4. Поддерживается ли прошивка Compute Modules через кластер? Yes. Only for the master node.\n5. USB connections to individual boards or is there a 1 Gbps switch or? По два вертикальных USB подсоединены каждый к своей ноде. Если смотреть со стороны портов: правый первый USB подсоединенем к мастер ноде, второй с ethernet ко второй ноде, третий к четвертой ноде и четвертый к шестой ноде\n6. I see ATX DC 12V power ports. Does this mean it can function from either an ATX power supply or 12V? Yes\n7. Guys what is the purpose of these things? Kind of\n To prototype and learn clustering applications and practices Host at the edge Kubernetes (k8s, k3s), Docker, TensorFlow, Caffe and other beauties of microservice architecture Home education lab Compute node for IoT purposes Home automation server  "
},
{
	"uri": "https://docs.turingpi.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/cmb/",
	"title": "cmb",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/cni/",
	"title": "cni",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/faq/",
	"title": "faq",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/helm/",
	"title": "helm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/i2c/",
	"title": "i2c",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/rpi/",
	"title": "rpi",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/rtc/",
	"title": "rtc",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/specs/",
	"title": "specs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/tags/turing_pi/",
	"title": "turing_pi",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/categories/wiki/",
	"title": "wiki",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://docs.turingpi.com/categories/wip/",
	"title": "wip",
	"tags": [],
	"description": "",
	"content": ""
}]